{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1babb6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from groq import Groq\n",
    "import os\n",
    "\n",
    "os.environ[\"GROQ_API_KEY\"] = \"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\"\n",
    "\n",
    "client = Groq()\n",
    "\n",
    "topic = \"Retrieval Augmented Generation\"\n",
    "prompt_text = f\"Explain {topic} in simple words.\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt_text}]\n",
    ")\n",
    "\n",
    "# Print the model's text reply (Groq's SDK returns ChatCompletionMessage objects)\n",
    "print(response.choices[0].message.content)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3437d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "model = ChatGroq(model=\"llama-3.1-8b-instant\")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"Explain {topic} in simple words.\"\n",
    ")\n",
    "chain = prompt | model\n",
    "\n",
    "print(chain.invoke({\"topic\": \"Retrieval Augmented Generation\"}).content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "48546e63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/zp/0npwwwts7j74m7psffzg4xwr0000gn/T/ipykernel_8321/1862862810.py:3: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the `langchain-huggingface package and should be used instead. To use it run `pip install -U `langchain-huggingface` and import as `from `langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedder = HuggingFaceEmbeddings(\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "embedder = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "21fb6b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [\n",
    "    \"Retrieval Augmented Generation (RAG) combines external knowledge with LLM generation.\",\n",
    "    \"Vector databases store embeddings that represent meaning instead of exact words.\",\n",
    "    \"Chunking splits large documents into smaller pieces to improve retrieval accuracy.\",\n",
    "    \"Embeddings are numerical representations of text that capture semantic meaning.\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "16967452",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={}, page_content='Retrieval Augmented Generation (RAG) combines'),\n",
       " Document(metadata={}, page_content='(RAG) combines external knowledge with LLM'),\n",
       " Document(metadata={}, page_content='knowledge with LLM generation.'),\n",
       " Document(metadata={}, page_content='Vector databases store embeddings that represent'),\n",
       " Document(metadata={}, page_content='that represent meaning instead of exact words.'),\n",
       " Document(metadata={}, page_content='Chunking splits large documents into smaller'),\n",
       " Document(metadata={}, page_content='into smaller pieces to improve retrieval'),\n",
       " Document(metadata={}, page_content='improve retrieval accuracy.'),\n",
       " Document(metadata={}, page_content='Embeddings are numerical representations of text'),\n",
       " Document(metadata={}, page_content='of text that capture semantic meaning.')]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=50,\n",
    "    chunk_overlap=20\n",
    ")\n",
    "\n",
    "documents = splitter.create_documents(docs)\n",
    "documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "36e00dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents,\n",
    "    embedding=embedder,\n",
    "    persist_directory=\"./chroma_db\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "73c467a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5234e0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGroq(model=\"llama-3.1-8b-instant\")\n",
    "\n",
    "rag_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "Use the following context to answer the question:\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer in a simple way.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "30fcac2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | rag_prompt\n",
    "    | llm\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3d1f40f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG combines external knowledge with a Large Language Model (LLM) generation to create new information.\n"
     ]
    }
   ],
   "source": [
    "response = rag_chain.invoke(\"What is RAG?\")\n",
    "print(response.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2803531e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An embedding is a numerical representation of text that captures its meaning.\n",
      "We chunk documents to improve retrieval accuracy.\n"
     ]
    }
   ],
   "source": [
    "print(rag_chain.invoke(\"What is an embedding?\").content)\n",
    "print(rag_chain.invoke(\"Why do we chunk documents?\").content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b424b586",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "384\n",
      "[-0.06957074254751205, 0.09520000964403152, 0.016021398827433586, 0.00680147111415863, -0.08840498328208923, 0.014204839244484901, 0.05402772128582001, 0.0456368550658226, -0.03192177787423134, -0.029563739895820618]\n"
     ]
    }
   ],
   "source": [
    "vec = embedder.embed_query(\"What is RAG?\")\n",
    "print(len(vec))\n",
    "print(vec[:10])  # show first 10 dims\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "12b6f096",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity: 0.3026949104640774\n"
     ]
    }
   ],
   "source": [
    "v1 = embedder.embed_query(\"RAG uses retrieval to help LLMs answer better.\")\n",
    "v2 = embedder.embed_query(\"Retrieval augmented generation combines docs and models.\")\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "cosine = np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n",
    "print(\"Cosine similarity:\", cosine)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e205d60",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (G-AI)",
   "language": "python",
   "name": "g-ai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
